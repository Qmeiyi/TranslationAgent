#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¯è¯­æå–è„šæœ¬ (Agent 1: Terminology & Entity Agent)
åŠŸèƒ½ï¼šè¯†åˆ«å¹¶è§„èŒƒåŒ–å°è¯´ä¸­çš„æœ¯è¯­ã€å®ä½“å’Œæ–‡åŒ–è´Ÿè½½è¯
å¢å¼ºç‰ˆï¼šæ”¯æŒå¤šç±»åˆ«æœ¯è¯­æå–ï¼ŒåŒ…å«è¯æ®ç‰‡æ®µå’Œå€™é€‰ç¿»è¯‘
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Union

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from utils.config import get_llm

PROJECT_ROOT = Path(__file__).resolve().parents[1]


class TermExtractionResult(BaseModel):
    """å•ä¸ªæœ¯è¯­æå–ç»“æœï¼ˆç”¨äºæå–é˜¶æ®µï¼‰"""
    term: str = Field(description="åŸæ–‡æœ¯è¯­")
    type: str = Field(
        description="ç±»å‹ï¼šNE:person, NE:location, NE:org, NE:work, slang, domain_term, culture_loaded"
    )
    meaning: Optional[str] = Field(default=None, description="æœ¯è¯­å«ä¹‰/å®šä¹‰")
    candidates: List[str] = Field(default_factory=list, description="å€™é€‰ç¿»è¯‘åˆ—è¡¨")
    evidence_span: str = Field(description="è¯¥æœ¯è¯­é¦–æ¬¡å‡ºç°æˆ–æœ€å…³é”®çš„åŸæ–‡ç‰‡æ®µå¼•ç”¨")
    chapter_id: Optional[int] = Field(default=None, description="ç« èŠ‚ID")
    chunk_id: Optional[str] = Field(default=None, description="chunk ID")


class TermExtractionOutput(BaseModel):
    """æœ¯è¯­æå–è¾“å‡ºï¼ˆæ‰¹é‡ï¼‰"""
    terms: List[TermExtractionResult]


class GlossaryEntry(BaseModel):
    """Glossaryä¸­çš„æ¡ç›®ï¼ˆç”¨äºæŒä¹…åŒ–ï¼‰"""
    term: str = Field(description="æºæœ¯è¯­")
    type: str = Field(description="ç±»å‹ï¼Œä¾‹å¦‚ NE:person, NE:location, NE:org, slang, domain_term")
    final: str = Field(description="æœ€ç»ˆè¯‘å")
    aliases: List[str] = Field(default_factory=list, description="åˆ«ååˆ—è¡¨")
    senses: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="å¤šä¹‰è¯ä¹‰é¡¹åˆ—è¡¨ï¼Œæ¯ä¸ªä¹‰é¡¹åŒ…å«sense_idã€context_signatureã€finalç­‰"
    )
    candidates: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="å€™é€‰ç¿»è¯‘åˆ—è¡¨ï¼ŒåŒ…å«translationã€scoreã€sourceç­‰"
    )
    evidence_span: Optional[str] = Field(default=None, description="è¯æ®ç‰‡æ®µ")
    score: Optional[float] = Field(default=None, description="è¯„åˆ†")


class Glossary(BaseModel):
    version: int = Field(default=1, description="æœ¯è¯­è¡¨ç‰ˆæœ¬")
    entries: List[GlossaryEntry]
    world_summary: Optional[str] = Field(default=None, description="ä¸–ç•Œè§‚æ‘˜è¦")


def _stringify_chapters(chapters: Iterable[dict]) -> str:
    parts: List[str] = []
    for chapter in chapters:
        chapter_id = chapter.get("chapter_id") or chapter.get("chapter_index") or "?"
        title = chapter.get("title") or ""
        text = chapter.get("text") or ""
        parts.append(f"[Chapter {chapter_id} {title}]\n{text}")
    return "\n\n".join(parts)


def load_full_text(input_path: Path) -> str:
    if input_path.suffix.lower() == ".jsonl":
        chapters = []
        with input_path.open("r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line)
                chapters.append(data)
        return _stringify_chapters(chapters)

    return input_path.read_text(encoding="utf-8")


def extract_terms(
    chapters_or_text: Union[str, Path, Iterable[dict]],
    output_path: Optional[Path] = None,
    dry_run: bool = False,
    max_chars: int = 60000,
) -> dict:
    if isinstance(chapters_or_text, Path):
        full_text = load_full_text(chapters_or_text)
    elif isinstance(chapters_or_text, str):
        full_text = chapters_or_text
    else:
        full_text = _stringify_chapters(chapters_or_text)

    if max_chars and len(full_text) > max_chars:
        full_text = full_text[:max_chars]

    if dry_run:
        glossary = {"version": 1, "entries": [], "world_summary": ""}
        if output_path:
            output_path.write_text(
                json.dumps(glossary, ensure_ascii=False, indent=2), encoding="utf-8"
            )
        return glossary

    llm = get_llm(temperature=0.1, max_tokens=6000)
    
    # ä½¿ç”¨TermExtractionOutputè¿›è¡Œæå–
    extraction_parser = JsonOutputParser(pydantic_object=TermExtractionOutput)
    
    system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å¥‡å¹»æ–‡å­¦ç¿»è¯‘æ€»ç›‘ï¼Œç†Ÿæ‚‰ã€Šè¯¡ç§˜ä¹‹ä¸»ã€‹ä¸–ç•Œè§‚ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ä»æ–‡æœ¬ä¸­æå–æ ¸å¿ƒæœ¯è¯­ï¼ŒåŒ…æ‹¬ï¼š
1. **å‘½åå®ä½“ï¼ˆNEï¼‰**ï¼š
   - NE:personï¼ˆäººåï¼Œå¦‚"å…‹è±æ©Â·è«é›·è’‚"ï¼‰
   - NE:locationï¼ˆåœ°åï¼Œå¦‚"å»·æ ¹å¸‚"ï¼‰
   - NE:orgï¼ˆç»„ç»‡åï¼Œå¦‚"å€¼å¤œè€…"ï¼‰
   - NE:workï¼ˆä½œå“åã€æ¦‚å¿µåï¼Œå¦‚"èµ«å¯†æ–¯æ–‡"ï¼‰

2. **ä¿šè¯­ï¼ˆslangï¼‰**ï¼šå°è¯´ä¸­çš„ç‰¹æ®Šè¡¨è¾¾ã€ä¿šè¯­

3. **é¢†åŸŸæœ¯è¯­ï¼ˆdomain_termï¼‰**ï¼šä¸–ç•Œè§‚ç›¸å…³çš„ä¸“ä¸šæœ¯è¯­ï¼Œå¦‚"éå‡¡è€…"ã€"é­”è¯"ã€"åºåˆ—"

4. **æ–‡åŒ–è´Ÿè½½è¯ï¼ˆculture_loadedï¼‰**ï¼šå…·æœ‰ç‰¹å®šæ–‡åŒ–å†…æ¶µçš„è¯æ±‡

å¯¹äºæ¯ä¸ªæœ¯è¯­ï¼Œè¯·æä¾›ï¼š
- term: åŸæ–‡æœ¯è¯­
- type: ç±»å‹ï¼ˆå¿…é¡»ä»ä¸Šè¿°ç±»åˆ«ä¸­é€‰æ‹©ï¼‰
- meaning: æœ¯è¯­çš„å«ä¹‰/å®šä¹‰
- candidates: å€™é€‰ç¿»è¯‘åˆ—è¡¨ï¼ˆè‡³å°‘2-3ä¸ªé€‰é¡¹ï¼‰
- evidence_span: è¯¥æœ¯è¯­é¦–æ¬¡å‡ºç°æˆ–æœ€å…³é”®çš„åŸæ–‡ç‰‡æ®µï¼ˆ50-100å­—ï¼‰

è¯·ç¡®ä¿æå–çš„æœ¯è¯­æ˜¯é‡è¦çš„ã€éœ€è¦å…¨ä¹¦ä¸€è‡´çš„ä¸“æœ‰åè¯å’Œå…³é”®æ¦‚å¿µã€‚"""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            (
                "user",
                "è¿™æ˜¯å°è¯´å†…å®¹ç‰‡æ®µã€‚è¯·æå–æœ¯è¯­å¹¶è¿”å›JSONæ ¼å¼ï¼š\n\n{full_text}\n\n{format_instructions}",
            ),
        ]
    )

    chain = prompt | llm | extraction_parser
    extraction_result = chain.invoke(
        {
            "full_text": full_text,
            "format_instructions": extraction_parser.get_format_instructions(),
        }
    )

    # å°†æå–ç»“æœè½¬æ¢ä¸ºGlossaryæ ¼å¼
    entries = []
    for term_data in extraction_result.get("terms", []):
        entry = GlossaryEntry(
            term=term_data.get("term", ""),
            type=term_data.get("type", ""),
            final=term_data.get("candidates", [""])[0] if term_data.get("candidates") else "",
            candidates=[
                {
                    "translation": cand,
                    "score": 1.0 - (i * 0.1),  # ç¬¬ä¸€ä¸ªå€™é€‰åˆ†æ•°æœ€é«˜
                    "source": "extraction"
                }
                for i, cand in enumerate(term_data.get("candidates", []))
            ],
            evidence_span=term_data.get("evidence_span"),
        )
        entries.append(entry)

    glossary = {
        "version": 1,
        "entries": [entry.model_dump() for entry in entries],
        "world_summary": "",
    }

    if output_path:
        output_path.write_text(
            json.dumps(glossary, ensure_ascii=False, indent=2), encoding="utf-8"
        )

    return glossary


def main() -> None:
    input_file = PROJECT_ROOT / "data" / "processed" / "è¯¡ç§˜ä¹‹ä¸»_final.jsonl"
    output_file = PROJECT_ROOT / "data" / "glossary" / "project_knowledge_base.json"

    print("ğŸ” å¯åŠ¨æœ¯è¯­æå–Agent...")
    output_file.parent.mkdir(parents=True, exist_ok=True)

    glossary = extract_terms(input_file, output_path=output_file)

    print(f"âœ¨ æå–æœ¯è¯­æ•°é‡: {len(glossary.get('entries', []))}")
    print(f"ğŸ’¾ çŸ¥è¯†åº“å·²ä¿å­˜è‡³ {output_file}")


if __name__ == "__main__":
    main()
