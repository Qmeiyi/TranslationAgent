#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¯è¯­æå–è„šæœ¬ (Agent 1: Terminology & Entity Agent)
åŠŸèƒ½ï¼šè¯†åˆ«å¹¶è§„èŒƒåŒ–å°è¯´ä¸­çš„æœ¯è¯­ã€å®ä½“å’Œæ–‡åŒ–è´Ÿè½½è¯
"""

import json
import os
from typing import List, Optional
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser

# --- 1. å®šä¹‰æ•°æ®ç»“æ„ ---
class TermEntry(BaseModel):
    """æœ¯è¯­æ¡ç›®æ•°æ®ç»“æ„"""
    term: str = Field(description="åŸæ–‡æœ¯è¯­")
    category: str = Field(description="ç±»åˆ«: Person, Location, Org, Concept, Item, Currency")
    definition: str = Field(description="ç»“åˆå…¨ä¹¦ä¸Šä¸‹æ–‡çš„æ·±åº¦å®šä¹‰")
    suggested_translation: str = Field(description="å»ºè®®çš„è‹±æ–‡è¯‘å (éœ€ä¿æŒå…¨ä¹¦ä¸€è‡´)")
    context_clue: Optional[str] = Field(description="è¯¥æœ¯è¯­é¦–æ¬¡å‡ºç°æˆ–æœ€å…³é”®çš„åŸæ–‡ç‰‡æ®µå¼•ç”¨", default=None)

class KnowledgeGraph(BaseModel):
    """çŸ¥è¯†åº“æ•°æ®ç»“æ„"""
    world_summary: str = Field(description="å¯¹å‰10ç« ä¸–ç•Œè§‚ã€åŠ›é‡ä½“ç³»çš„ç®€è¦æ€»ç»“ï¼ˆ200å­—ä»¥å†…ï¼‰")
    terms: List[TermEntry]

# --- 2. åˆå§‹åŒ–æ¨¡å‹ --- 
def init_llm():
    """åˆå§‹åŒ–LLMæ¨¡å‹"""
    return ChatOpenAI(
        model="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
        api_key="sk-cautwxmuhdpxhtuilctlfpecaoxpzhagpzfzmkdxgrywjpum", 
        base_url="https://api.siliconflow.cn/v1/",
        temperature=0.1,
        max_tokens=8000
    )

# --- 3. åŠ è½½æ–‡æœ¬ --- 
def load_full_text(filepath: str) -> str:
    """
    åŠ è½½å®Œæ•´æ–‡æœ¬ç”¨äºæœ¯è¯­æå–
    
    Args:
        filepath: æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ‹¼æ¥å¥½çš„å®Œæ•´æ–‡æœ¬
    """
    full_text = ""
    print("ğŸ“š æ­£åœ¨åŠ è½½å…¨é‡æ–‡æœ¬åˆ°å†…å­˜...")
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            # æ‹¼æ¥æ ¼å¼ï¼š [ç¬¬Xç«  æ ‡é¢˜] 
            full_text += f"\n\n[ç¬¬{data['chapter_index']}ç«  {data['title']}]\n{data['text']}"
            full_text += f"\n\n[ç¬¬{data['chapter_index']}ç«  {data['title']}]\n{data['text']}"
    
    token_est = len(full_text) 
    print(f"âœ… åŠ è½½å®Œæˆï¼æ€»å­—ç¬¦æ•°: {len(full_text)} (é¢„ä¼° Token: {token_est // 1.5:.0f})")
    print(f"   æ­¤é•¿åº¦å®Œå…¨åœ¨ DeepSeek 128K (çº¦ 12.8ä¸‡ Token) è¦†ç›–èŒƒå›´å†…ã€‚")
    return full_text

# --- 4. ä¸»ç¨‹åº --- 
def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œæœ¯è¯­æå–æµç¨‹
    """
    # é…ç½®è·¯å¾„
    input_file = "../data/processed/è¯¡ç§˜ä¹‹ä¸»_final.jsonl"
    output_file = "../data/glossary/project_knowledge_base.json"
    
    print("ğŸ” å¯åŠ¨æœ¯è¯­æå–Agent...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # 1. åˆå§‹åŒ–æ¨¡å‹å’Œè§£æå™¨
    llm = init_llm()
    parser = JsonOutputParser(pydantic_object=KnowledgeGraph)
    
    # 2. è®¾è®¡æç¤ºè¯
    system_prompt = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å¥‡å¹»æ–‡å­¦ç¿»è¯‘æ€»ç›‘ã€‚ä½ æ‹¥æœ‰è¿‡ç›®ä¸å¿˜çš„èƒ½åŠ›ï¼Œå·²é˜…è¯»äº†ã€Šè¯¡ç§˜ä¹‹ä¸»ã€‹çš„å‰10ç« å…¨æ–‡ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯æ„å»ºä¸€ä»½**â€œæ ¸å¿ƒæœ¯è¯­ä¸ä¸–ç•Œè§‚æŒ‡å—â€**ï¼Œä»¥ç¡®ä¿åç»­ç¿»è¯‘çš„ç»Ÿä¸€æ€§ã€‚
    
    è¯·åˆ©ç”¨ä½ å¯¹å…¨ä¹¦çš„ç†è§£ï¼š
    1. **å»é‡ä¸åˆå¹¶**ï¼šåŒä¸€ä¸ªå®ä½“ï¼ˆå¦‚â€œå‘¨æ˜ç‘â€å’Œâ€œå…‹è±æ©â€ï¼‰å¦‚æœæ˜¯æŒ‡å‘åŒä¸€äººï¼Œè¯·åœ¨å®šä¹‰ä¸­è¯´æ˜ï¼Œä½†ä¿ç•™ä¸»è¦ç§°å‘¼ä½œä¸ºæœ¯è¯­ã€‚
    2. **æ·±åº¦ç†è§£**ï¼šå¯¹äºâ€œéå‡¡è€…â€ã€â€œé­”è¯â€ç­‰æ ¸å¿ƒè®¾å®šï¼Œä¸è¦åªçœ‹å­—é¢æ„æ€ï¼Œè¦ç»“åˆä¸Šä¸‹æ–‡æ€»ç»“å…¶åœ¨æœ¬ä¹¦ä¸­çš„ç‰¹æ®Šå«ä¹‰ã€‚
    3. **è‹±æ–‡å‘½å**ï¼šå¯¹äºäººååœ°åï¼Œå‚è€ƒç»´å¤šåˆ©äºšæ—¶ä»£é£æ ¼ï¼ˆVictorian Styleï¼‰ï¼›å¯¹äºä¸“æœ‰åè¯ï¼Œå‚è€ƒå…‹è‹é²ç¥è¯ï¼ˆCthulhu Mythosï¼‰é£æ ¼ã€‚
    
    è¯·æ³¨æ„ï¼šè¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ª JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„åˆ†ææ–‡æœ¬ã€‚
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "è¿™æ˜¯å°è¯´çš„å‰ 10 ç« å®Œæ•´å†…å®¹ï¼ˆå…±çº¦ 2 ä¸‡å­—ï¼‰ã€‚è¯·åˆ†æå¹¶æå–æ ¸å¿ƒçŸ¥è¯†åº“ï¼š\n\n{full_text}\n\n{format_instructions}")
    ])
    
    chain = prompt | llm | parser
    
    # 3. å‡†å¤‡é•¿æ–‡æœ¬
    full_context = load_full_text(input_file)
    
    # 4. è°ƒç”¨å¤§æ¨¡å‹
    print("\nğŸš€ æ­£åœ¨å‘é€è¯·æ±‚ç»™ DeepSeek (è¿™å¯èƒ½éœ€è¦ 30-60 ç§’)...")
    try:
        result = chain.invoke({
            "full_text": full_context,
            "format_instructions": parser.get_format_instructions()
        })
        
        # 5. ç»“æœå±•ç¤ºä¸ä¿å­˜
        print("\nâœ¨ ä¸–ç•Œè§‚æ€»ç»“:")
        print(result['world_summary'])
        print(f"\nâœ¨ æå–æœ¯è¯­æ•°é‡: {len(result['terms'])}")
        
        # æ‰“å°å‰å‡ ä¸ªçœ‹çœ‹
        for term in result['terms'][:5]:
            print(f"   - {term['term']} ({term['suggested_translation']}): {term['definition']}")

        # ä¿å­˜ä¸ºæœ€ç»ˆçŸ¥è¯†åº“
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ çŸ¥è¯†åº“å·²ä¿å­˜è‡³ {output_file}")
            
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()